%!TEX root = ../draft.tex
\chapter{Diskussion}\label{s.diskussion}
In diesem Kapitel sollen die Ergebnisse und Erkenntnisse, welche im Verlauf der Arbeit herausgearbeitet wurden, kurz aufgeführt und erläutert werden.
\section{Datensätze und Netze}
Die Daten der trainierten Netze zeigen, dass mit den verwendeten Datensätzen und den angewandten Normalisierungsmethoden keine Erhöhung der Genauigkeit erzielt werden konnte. Ein möglicher Grund sind die Datensätze, welche normalisiert wurden. Da manche Normalisierungsverfahren die Farbinformationen des gesamten Bildes verwendeten, ist das Ergebnis der Normalisierungen, je nach Umgebung, in den Bildern unterschiedlich ausgefallen. Um diese Annahme zu überprüfen, wurden Testaufnahmen von Objekten auf verschiedenen Hintergründen mit gleicher Belichtung und gleichem Aufnahmewinkel erstellt. In den unveränderten Testbildern ist kein großer Unterschied in den Histogrammen aufgefallen. Bei gleicher Lichteinstrahlung und unterschiedlichen Hintergründen haben die Objekte eine fast identische Farbverteilung. Lediglich die Helligkeit hat etwas variiert. Der normalisierte Datensatz jedoch zeigt größere Unterschiede in den Histogrammen und bei den Farben. Das könnte daran liegen, dass die Normalisierung nicht nur die Objekte auf den Bildern normalisiert, sondern auch den Hintergrund, welcher bei vielen der Trainingsbilder unterschiedlich ist.\\ 
Die Überprüfung hat gezeigt, dass die Datensätze für den verwendeten Aufbau und den verwendeten Normalisierungsalgorithmen nicht geeignet waren. Bessere Ergebnisse konnten mit dem im nachhinein generierten Obst-Datensatz erzielt werden, bei dem alle Trainingsdaten auf demselben Hintergrund aufgenommen wurden. Dadurch konnten alle Klassen auf derselben Basis normalisiert werden. Gerade die Histogramm-Spezifikation (Abbildung \ref{img:hellver}) hat hier besonders gute Ergebnisse erzielt, da der Einsatz eines Referenzbildes besser funktioniert hat und wesentlich weniger Anomalien erzeugt wurden. Auch bei der Untersuchung der Helligkeit konnten wesentliche Verbesserungen festgestellt werden. Bei einem stark unterbelichteten Bild wurde, durch den Histogramm-Ausgleich, die Lichtverteilung wesentlich angehoben. Die Farbinformationen konnten jedoch größtenteils nicht wiederhergestellt werden. Durch die Histogramm-Spezifikation hat sich auch die Lichtverteilung verbessert und durch das Referenzbild wurden zudem die Farbinformationen wiederhergestellt. Der Gray-World-Algorithmus schneidet hier am schlechtesten ab, da dieser nicht mit den einzelnen Farbwerten arbeitet, sondern die gesamten Farbkanäle auf Basis des Grünkanals verschiebt und nur Lichtfarben ausgleicht. 
\section{Laufzeittest}
Ein weiterer Einfluss, welchen die Normalisierungsalgorithmen auf die Klassifizierung von neuronalen Netzen haben, ist, neben dem Manipulieren von Trainingsdaten, die Zeit welche das jeweilige Verfahren benötigt. Um herauszufinden, wie sich die Laufzeit bei größer werdenden Bilddateien verhält, wurde ein Bild in verschiedenen Größen getestet.
\begin{table}
[h]
\caption{Laufzeiten der Normalisierungs-Algorithmen mit verschieden großen Bildern}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Bildgröße & Faktor & Dateigröße & GW(s) & HA(s) & HS(s)\\
\hline
4160x3120 & 100\% & 2.441 KB & 1,447s & 0,172s & 6,952s\\
2912x2184 & 70\% & 1.000 KB & 0,755s & 0,078s & 3,215s\\
2080x1560 & 50\% & 529 KB & 0,362s & 0,035s & 1,717s\\
1040x780 & 25\% & 114 KB & 0,094s & 0,013s & 0,422s\\
\hline
\end{tabular}
\end{table}
Der Gray-World-Algorithmus liegt, im Vergleich zu den anderen Methoden, bei der Laufzeit im Mittelfeld. Bei Verdoppelung der Bildgröße erhöht sich die Laufzeit um das Vierfache. Dabei sind sowohl die Farbgebung, als auch der Kontrast nicht entscheidend. Die Histogramm-Ausgleichung ist in diesem Fall die schnellste Methode, wobei hierbei die Verteilung im Histogramm eine Rolle spielt. Die Laufzeit verändert sich nicht linear, sondern steigt exponentiell an. So steigt die Laufzeit beim Verdoppeln um den Faktor drei und beim nächsten Mal um den Faktor sechs. Der längste Normalisierungsalgorithmus ist in diesem Fall die Histogramm-Spezifikation. Dabei spielt die Größe des Referenzbildes eine wichtige Rolle. Mit einem größeren Referenzbild würde sich die Laufzeit noch weiter erhöhen. Bei einem gleichbleibenden Referenzbild erhöht sich die Laufzeit um den Faktor vier. Durch die hohe Grundlaufzeit führt das schnell zu langen Laufzeiten.\\\\
Hierbei muss beachtet werden, dass die Laufzeiten nur für ein Bild berechnet wurden. Für ein künstliches neuronales Netz werden meist mehrere Tausend Bilder verwendet. Hier muss, je nach Datensatz, abgewägt werden, ob die erhöhte Laufzeit für eine Normalisierung in Kauf genommen werden soll.
