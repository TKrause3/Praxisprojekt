%!TEX root = ../draft.tex
\chapter{Diskussion}\label{s.diskussion}
In diesem Kapitel sollen die Ergebnisse und Erkenntnisse, welche im Verlauf der Arbeit herausgearbeitet wurden, kurz aufgeführt und erläutert werden.
\section{Datensätze und Netze}
Die Daten der trainierten Netze zeigen, dass mit den verwendeten Datensätzen und den angewandten Normalisierungsmethoden keine Erhöhung der Genauigkeit erzielt werden konnte. Ein möglicher Grund waren die Datensätze selber, welche normalisiert wurden. Da manche Normalisierungsverfahren die Farbinformationen des gesamten Bildes verwendeten, ist das Ergebnis der Normalisierungen, je nach Umgebung, in den Bildern, unterschiedlich ausgefallen. Um diese Annahme zu überprüfen wurden Testaufnahmen von Objekten auf verschieden Hintergründen aufgenommen, welche aber gleiche Belichtung und Aufnahmewinkel hatten. In den unveränderten Testbildern ist kein großer Unterschied in den Histogrammen aufgefallen. Bei gleicher Lichteinstrahlung und unterschiedlichen Hintergründen, haben die Objekte eine fast identische Farbverteilung. Lediglich die Helligkeit hat etwas variiert. Der normalisierte Datensatz jedoch, zeigt größere Unterschiede in den Histogrammen und den Farben auf. Das könnte daran liegen, dass die Normalisierung nicht nur die Objekte auf den Bildern normalisiert, sondern auch den Hintergrund, welcher bei vielen der Trainingsbilder unterschiedlich ist.\\ 
Die Überprüfung hat gezeigt, das die Datensätze für den verwendeten Aufbau und den verwendeten Normalisierungsalgorithmen nicht geeignet waren. Bessere Ergebnisse konnten mit dem, im nachhinein, generierten Obst-Datensatz erzielt werden, bei dem alle Trainingsdaten auf demselben Hintergrund aufgenommen wurden. Dadurch konnten alle Klassen auf derselben Basis normalisiert werden. Gerade die Histogramm-Spezifikation (Abbildung \ref{img:hellver}) hat hier besonders gute Ergebnisse erzielt, da der Einsatz eines Referenzbildes besser funktioniert, und wesentlich weniger Anomalien erzeugt wurden. Auch bei Untersuchung der Helligkeit konnten wesentliche Verbesserungen erzielt werden. Bei einem stark unterbelichteten Bild wurde durch den Histogramm-Ausgleich die Lichtverteilung wesentlich angehoben. Die Farbinformationen konnten jedoch größtenteils nicht wiederhergestellt werden. Durch die Histogramm-Spezifikation hat sich auch die Lichtverteilung verbessert und durch das Referenzbild wurden auch die Farbinformationen wiederhergestellt. Der Gray-World-Algorithmus schneidet hier am schlechtesten ab, da dieser nicht mit den einzelnen Farbwerten arbeitet, sondern die gesamten Farbkanäle auf Basis des Grün-Kanals verschiebt und nur Lichtfarben ausgleicht. 
\section{Laufzeittest}
Ein weiterer Einfluss, welchen die Normalisierungsalgorithmen auf die Klassifizierung von neuronalen Netzen haben, ist neben dem Manipulieren von Trainingsdaten, die Zeit welche die jeweiligen Verfahren benötigen. Um einen Eindruck zu bekommen, wurde ein Bild in verschiedenen Größen getestet, um herauszufinden, wie sich die Laufzeit bei größer werdenden Bilddateien verhält.
\begin{table}
[h]
\caption{Laufzeiten der Normalisierungs-Algorithmen mit verschieden großen Bildern}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Bildgröße & Faktor & Dateigröße & GW(s) & HA(s) & HS(s)\\
\hline
4160x3120 & 100\% & 2.441 KB & 1,447s & 0,172s & 6,952s\\
2912x2184 & 70\% & 1.000 KB & 0,755s & 0,078s & 3,215s\\
2080x1560 & 50\% & 529 KB & 0,362s & 0,035s & 1,717s\\
1040x780 & 25\% & 114 KB & 0,094s & 0,013s & 0,422s\\
\hline
\end{tabular}
\end{table}
Der Gray-World-Algorithmus ist im Vergleich zu den anderen Methoden in der Mitte von der Laufzeit, bei Verdoppelung der Bildgröße erhöht sich die Laufzeit um das Vierfache. Dabei sind auch die Farbgebung und der Kontrast nicht entscheidend. Die Histogramm-Ausgleichung ist in diesem Fall die schnellste Methode, wobei dafür die Verteilung im Histogramm eine Rolle spielt. Die Laufzeit verändert sich nicht linear, sondern steigt exponentiell an. So steigt die Laufzeit beim Verdoppeln um den Faktor drei und beim nächsten Mal um den Faktor sechs. Der längste Normalisierungsalgorithmus ist in diesem Fall die Histogramm-Spezifikation. Dabei spielt die Größe des Referenzbildes eine wichtige Rolle. Mit einem größeren Referenzbild würde sich die Laufzeit noch weiter erhöhen. Bei gleichbleibendem Referenzbild erhöht sich die Laufzeit um den Faktor vier. Durch die hohe Grundlaufzeit führt das schnell zu langen Laufzeiten.\\\\
Dabei muss beachtet werden, dass die Laufzeiten nur für ein Bild berechnet wurde. Für ein künstliches neuronales Netz werden meist mehrere tausend Bilder verwendet. Hier muss, je nach Datensatz, abgewägt werden, ob eine Normalisierung für die erhöhte Laufzeit in Kauf genommen werden soll.  